{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b179f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api is loaded\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_APIKEY\")\n",
    "print(\"Api is loaded\") if api_key else print(\"Api is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bc7e0",
   "metadata": {},
   "source": [
    "#### Openai agent sdk with groq key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d30d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 response :  I am calling from Saginfotech. Are you interested in Eazemax and Gencodex software tools? We specialize in providing innovative solutions to streamline business processes and improve productivity. Our software tools can help you automate tasks, enhance data management, and gain valuable insights to drive growth.\n",
      "\n",
      "If you're not familiar with Eazemax and Gencodex, I'd be happy to explain their features and benefits, and explore how they can address your specific needs. We've seen significant success with our existing clients, and I believe our solutions can bring value to your organization as well.\n",
      "\n",
      "What type of business or industry are you in, and what challenges are you currently facing that our software tools might be able to help with? Or do you have any specific questions about Eazemax and Gencodex that I can address?\n",
      "Agent 2 response :  Namaste! Main Saginfotech se baat kar raha hu. Ky app Eazemax Aur Gencodex software main interested hai?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    }
   ],
   "source": [
    "# required if in case of run in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "from agents import Agent, Runner, function_tool, OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "groq_api_key = api_key #os.getenv('GROQ_API_KEY')\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "groq_client = AsyncOpenAI(base_url=GROQ_BASE_URL, api_key=groq_api_key)\n",
    "llama3_3_model = OpenAIChatCompletionsModel(model=\"llama-3.3-70b-versatile\", openai_client=groq_client)\n",
    "\n",
    "\n",
    "agent1 = Agent(name=\"Lead Calling Agent English\", instructions=\"You are a helpful calling Agent. Your will start the conversation You will say I am calling from Saginfotech. Are you interested in eazemax and gencodex software tool. and write back response of user query if you have any prier knowledge \",model=llama3_3_model)\n",
    "\n",
    "agent2 = Agent(name=\"Lead Calling Agent Hindi\", instructions=\"You are a helpful calling Agent. Your will start the conversation  in Hindi You will say 'Main Saginfotech se baat kar raha hu . Ky app Eazemax Aur Gencodex software main interested hai. \", model = llama3_3_model )\n",
    "\n",
    "result1 = Runner.run_sync(agent1,\"start the conversation, what you can do for me?\")\n",
    "result2 = Runner.run_sync(agent2,\"start the conversation\")\n",
    "print(\"Agent 1 response : \",result1.final_output)\n",
    "print(\"Agent 2 response : \",result2.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2badabf3",
   "metadata": {},
   "source": [
    "#### Groq api calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe387db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - {'error': {'message': 'llama-3.3-70b-versatile is currently over capacity. Please try again and back off exponentially. Visit https://groqstatus.com to see if there is an active incident.', 'type': 'internal_server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[0;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m Groq(api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[1;32m----> 5\u001b[0m chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Set an optional system message. This sets the behavior of the\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# assistant and can be used to provide specific instructions for\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# how it should behave throughout the conversation.\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Set a user message for the assistant to respond to.\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExplain the importance of fast language models\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The language model which will generate the completion.\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.3-70b-versatile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Print the completion returned by the LLM.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32md:\\Learning Folder\\openai-agent-sdk\\agent\\lib\\site-packages\\groq\\resources\\chat\\completions.py:368\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    229\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minclude_domains\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_settings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Learning Folder\\openai-agent-sdk\\agent\\lib\\site-packages\\groq\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Learning Folder\\openai-agent-sdk\\agent\\lib\\site-packages\\groq\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 503 - {'error': {'message': 'llama-3.3-70b-versatile is currently over capacity. Please try again and back off exponentially. Visit https://groqstatus.com to see if there is an active incident.', 'type': 'internal_server_error'}}"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2781b",
   "metadata": {},
   "source": [
    "#### openAi with groq api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516ca53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff, the President of India is:\n",
      "\n",
      "**Draupadi Murmu**\n",
      "\n",
      "She is the 15th President of India and took office on July 25, 2022.\n",
      "\n",
      "And the Vice President of India is:\n",
      "\n",
      "**Jagdeep Dhankhar**\n",
      "\n",
      "He is the 14th Vice President of India and took office on August 11, 2022.\n",
      "\n",
      "Please note that these positions are subject to change, and if you need more up-to-date information, I can try to help you with that!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# âœ… Define agent-like behavior using Groq's LLaMA 3 model\n",
    "client = openai.OpenAI(base_url=url,api_key=api_key)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"who is the president and vice precident of india?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9366f15",
   "metadata": {},
   "source": [
    "#### OpenAi Agent SDK Development (handoffs primitive define in agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e637fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question :  who was the prime minister of india in 2015?\n",
      "answer :  In 2015, the Prime Minister of India was **Narendraâ€¯Modi**. He assumed office on 26â€¯Mayâ€¯2014 after the Bharatiya Janata Party (BJP) won a decisive victory in the general elections. Modiâ€™s first term (2014â€‘2019) was marked by a focus on economic reforms, digital initiatives, and efforts to boost foreign investment, setting the stage for the continued development agenda that he carried on in subsequent terms.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "groq_client = AsyncOpenAI(base_url=GROQ_BASE_URL, api_key=api_key)\n",
    "open_ai_model = \"openai/gpt-oss-20b\"\n",
    "llama3_3_model = OpenAIChatCompletionsModel(model=open_ai_model, openai_client=groq_client)\n",
    "\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model = llama3_3_model\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\", \n",
    "    model = llama3_3_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent], \n",
    "    model = llama3_3_model)\n",
    "\n",
    "\n",
    "\n",
    "query = input(\"Ask your query .................    \")\n",
    "print(\"question : \",query)\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, query)\n",
    "    print(\"answer : \",result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950c200",
   "metadata": {},
   "source": [
    "#### OpenAi Agent SDK Development Add a guardrail with handoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e12fafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Response: The first president of the United States was **Georgeâ€¯Washington**. He was inaugurated on Aprilâ€¯30,â€¯1789, and served two terms until 1797.\n",
      "\n",
      "### Why he was chosen\n",
      "- **Reputation and leadership:** Washington had led the Continental Army to victory in the Revolutionary War and was widely respected for his integrity and military skill.\n",
      "- **Neutrality:** As a relatively young nation, the United States needed a president who could appeal to a broad range of factions and maintain political stability.\n",
      "- **Constitutional guidance:** The newly drafted Constitution set up a system of checks and balances; Washingtonâ€™s leadership helped define the executiveâ€™s role within that framework.\n",
      "\n",
      "### Key actions during his presidency\n",
      "- **Establishing precedents:** Washington set many of the protocols still used todayâ€”such as appointing a cabinet, issuing the first executive orders, and laying out a neutral foreign policy stance.\n",
      "- **Neutrality Proclamation (1793):** Declared the U.S. would not take sides in the conflict between Britain and France, helping keep the young nation out of European wars.\n",
      "- **Farewell Address (1796):** Warned against political parties and entangling alliances, influencing American political thought for decades.\n",
      "\n",
      "Washingtonâ€™s presidency laid the foundations for the U.S. government and helped shape the nationâ€™s early direction.\n",
      "ðŸ¤– Response: To find the square of a number, you multiply the number by itself.\n",
      "\n",
      "**Stepâ€‘byâ€‘step for 2:**\n",
      "\n",
      "1. Take the number 2.  \n",
      "2. Multiply it by itself:  \n",
      "   \\[\n",
      "   2 \\times 2 = 4\n",
      "   \\]\n",
      "3. The result is 4.\n",
      "\n",
      "So, **the square of 2 is 4**.\n",
      "\n",
      "---\n",
      "\n",
      "### Quick examples to reinforce the idea\n",
      "\n",
      "| Number | Square (Number Ã— Number) | Result |\n",
      "|--------|--------------------------|--------|\n",
      "| 1      | \\(1 \\times 1\\)           | 1      |\n",
      "| 3      | \\(3 \\times 3\\)           | 9      |\n",
      "| 5      | \\(5 \\times 5\\)           | 25     |\n",
      "\n",
      "In each case, you just multiply the number by itself to get the square.\n",
      "ðŸ¤– Deny Response  : Iâ€™m sorry, but I canâ€™t help with that.  \n",
      "Iâ€™m only able to provide assistance on math and history topics.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner, InputGuardrailTripwireTriggered\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"You are asked to check if this is query is related to (Math,History) query. Answer clearly in text: 'Yes â€“ reason...' or 'No â€“ reason...'\",\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "\n",
    "send_deny_response_agent = Agent(\n",
    "    name=\"Deny replier\",\n",
    "    instructions=\"You are responsible to the write deny response message of query if not related to 'Math and History', reply polietly, and also write reason why you can not help with that query. \",\n",
    "    model=llama3_3_model)\n",
    "\n",
    "\n",
    "\n",
    "async def guardrail_function(ctx, agent, user_input):\n",
    "    result = await Runner.run(guardrail_agent, user_input, context=ctx.context)\n",
    "    raw = result.final_output.strip().lower()\n",
    "    # print(raw)\n",
    "    is_valid_query = raw.startswith(\"yes\")\n",
    "    reasoning = result.final_output.strip()\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info={\"is_homework\": is_valid_query, \"reasoning\": reasoning},\n",
    "        tripwire_triggered=not is_valid_query\n",
    "    )\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"Route homework questions to the right tutor, get response frome the expert tutor.\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    input_guardrails=[InputGuardrail(guardrail_function=guardrail_function)],\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    for q in [\"who was the first president of the united states?\", \"what is the square of 2?\",\"Name of bollywood 3 khan superstar?\"]:\n",
    "        try:\n",
    "            res = await Runner.run(triage_agent, q)\n",
    "            print(f\"ðŸ¤– Response: {res.final_output}\")\n",
    "        except InputGuardrailTripwireTriggered:\n",
    "            # Use the guardrail result's reasoning to craft a reply\n",
    "            res = await Runner.run(send_deny_response_agent,f\"User is asked this question : {q}\") #  <---- \n",
    "            print(f\"ðŸ¤– Deny Response  : {res.final_output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f8100",
   "metadata": {},
   "source": [
    "#### openAi Agent with Tool configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e710277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s the current weather in **Mumbai**:\n",
      "\n",
      "- **Temperature:** 27.5â€¯Â°C (â‰ˆ81â€¯Â°F)  \n",
      "- **Humidity:** 80â€¯%  \n",
      "- **Conditions:** Overcast clouds  \n",
      "\n",
      "Feel free to let me know if youâ€™d like a forecast for the next few days or weather for another city!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import requests\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool, set_tracing_disabled\n",
    "\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "API_KEY = \"25f0559351f3c75faa45800ee42df5b1\"  # ðŸ”‘ Replace with your key\n",
    "BASE_URL = \"https://api.openweathermap.org/data/2.5/weather\"  # login signup and create api\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": \"metric\"  # Use \"imperial\" for Fahrenheit\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        # print(data)\n",
    "        if response.status_code == 200:\n",
    "            # print({\n",
    "            #     \"city\": data[\"name\"],\n",
    "            #     \"temperature\": data[\"main\"][\"temp\"],\n",
    "            #     \"humidity\": data[\"main\"][\"humidity\"],\n",
    "            #     \"description\": data[\"weather\"][0][\"description\"].capitalize()\n",
    "            # })\n",
    "            return {\n",
    "                \"city\": data[\"name\"],\n",
    "                \"temperature\": data[\"main\"][\"temp\"],\n",
    "                \"humidity\": data[\"main\"][\"humidity\"],\n",
    "                \"description\": data[\"weather\"][0][\"description\"].capitalize()\n",
    "            }\n",
    "        else:\n",
    "            return {\"error\": data.get(\"message\", \"Unable to fetch weather data\")}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # This agent will use the custom LLM provider\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"You are responsive to provide the whether information to a specific, using tool.\",\n",
    "        model=  llama3_3_model,  \n",
    "        tools=[get_weather],\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(agent, \"What's the weather in mummai?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188a647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
