{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b179f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api is loaded\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_APIKEY\")\n",
    "print(\"Api is loaded\") if api_key else print(\"Api is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bc7e0",
   "metadata": {},
   "source": [
    "#### Openai agent sdk with groq key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d30d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 response :  I am calling from Saginfotech. Are you interested in Eazemax and Gencodex software tools? We specialize in providing innovative software solutions to streamline business operations and improve efficiency. Our Eazemax and Gencodex tools are designed to help organizations like yours automate tasks, enhance productivity, and make data-driven decisions. Would you like to know more about how our software can benefit your business?\n",
      "Agent 2 response :  Main Saginfotech se baat kar raha hu . Ky app Eazemax Aur Gencodex software main interested hai?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    }
   ],
   "source": [
    "# required if in case of run in jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "from agents import Agent, Runner, function_tool, OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "groq_api_key = api_key #os.getenv('GROQ_API_KEY')\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "groq_client = AsyncOpenAI(base_url=GROQ_BASE_URL, api_key=groq_api_key)\n",
    "llama3_3_model = OpenAIChatCompletionsModel(model=\"llama-3.3-70b-versatile\", openai_client=groq_client)\n",
    "\n",
    "\n",
    "agent1 = Agent(name=\"Lead Calling Agent English\", instructions=\"You are a helpful calling Agent. Your will start the conversation You will say I am calling from Saginfotech. Are you interested in eazemax and gencodex software tool. and write back response of user query if you have any prier knowledge \",model=llama3_3_model)\n",
    "\n",
    "agent2 = Agent(name=\"Lead Calling Agent Hindi\", instructions=\"You are a helpful calling Agent. Your will start the conversation  in Hindi You will say 'Main Saginfotech se baat kar raha hu . Ky app Eazemax Aur Gencodex software main interested hai. \", model = llama3_3_model )\n",
    "\n",
    "result1 = Runner.run_sync(agent1,\"start the conversation, what you can do for me?\")\n",
    "result2 = Runner.run_sync(agent2,\"start the conversation\")\n",
    "print(\"Agent 1 response : \",result1.final_output)\n",
    "print(\"Agent 2 response : \",result2.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2badabf3",
   "metadata": {},
   "source": [
    "#### Groq api calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe387db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial in today's technological landscape, and their importance can be understood from several perspectives:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models can process and analyze vast amounts of text data quickly, which is essential for applications that require real-time or near-real-time responses, such as chatbots, virtual assistants, and language translation software. This speed enables these models to handle a high volume of requests without significant delays.\n",
      "\n",
      "2. **Resource Optimization**: By being fast, these models can operate on less powerful hardware or use less computational resources (like GPU power) to achieve the same task as slower models. This optimization is vital for deploying language models on edge devices (like smartphones or smart home devices) where computational resources are limited. It also helps in reducing the energy consumption and environmental impact of data centers.\n",
      "\n",
      "3. **Real-time Interaction**: The ability to respond quickly is essential for creating interactive and engaging user experiences. In applications like voice assistants, fast language models allow for almost instantaneous responses to voice commands, making the interaction feel more natural and intuitive.\n",
      "\n",
      "4. **Competitive Advantage**: For businesses, the speed of language models can be a significant competitive advantage. Faster models can analyze customer feedback, generate responses, and personalize content more quickly than slower models, leading to better customer satisfaction and retention.\n",
      "\n",
      "5. **Innovation and Research**: Fast language models facilitate more rapid experimentation and innovation in the field of natural language processing (NLP). Researchers can quickly test hypotheses, train models, and iterate on their designs, which accelerates the development of new technologies and applications.\n",
      "\n",
      "6. **Accessibility**: Fast and efficient language models can also improve accessibility by providing quicker and more accurate translations, text-to-speech, and speech-to-text functionalities. This can significantly benefit individuals with disabilities, language barriers, or those in need of instant information access.\n",
      "\n",
      "7. **Scalability**: As the amount of text data grows exponentially, fast language models are better equipped to handle this scalability challenge. They can process large volumes of data more efficiently, making them indispensable for big data analytics, social media monitoring, and other applications that deal with massive text datasets.\n",
      "\n",
      "8. **Cost Reduction**: In a commercial context, the speed of language models can lead to cost savings. Faster processing times mean that fewer resources are needed to achieve the same outcomes, which can reduce operational costs. Additionally, the faster development and deployment of NLP solutions can accelerate the time-to-market for new products and services, potentially generating revenue sooner.\n",
      "\n",
      "In summary, fast language models are vital for enhancing user experience, optimizing resources, driving innovation, and improving accessibility, among other benefits. As technology continues to evolve, the importance of speed in language models will only continue to grow.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\"\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2781b",
   "metadata": {},
   "source": [
    "#### openAi with groq api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516ca53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff, the President of India is:\n",
      "\n",
      "**Draupadi Murmu**\n",
      "\n",
      "She is the 15th President of India and took office on July 25, 2022.\n",
      "\n",
      "And the Vice President of India is:\n",
      "\n",
      "**Jagdeep Dhankhar**\n",
      "\n",
      "He is the 14th Vice President of India and took office on August 11, 2022.\n",
      "\n",
      "Please note that these positions are subject to change, and if you need more up-to-date information, I can try to help you with that!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# ✅ Define agent-like behavior using Groq's LLaMA 3 model\n",
    "client = openai.OpenAI(base_url=url,api_key=api_key)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"who is the president and vice precident of india?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9366f15",
   "metadata": {},
   "source": [
    "#### OpenAi Agent SDK Development (handoffs primitive define in agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7e637fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question :  who was the prime minister of india in 2015?\n",
      "answer :  In 2015, the Prime Minister of India was **Narendra Modi**. He assumed office on 26 May 2014 after the Bharatiya Janata Party (BJP) won a decisive victory in the general elections. Modi’s first term (2014‑2019) was marked by a focus on economic reforms, digital initiatives, and efforts to boost foreign investment, setting the stage for the continued development agenda that he carried on in subsequent terms.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "groq_client = AsyncOpenAI(base_url=GROQ_BASE_URL, api_key=api_key)\n",
    "open_ai_model = \"openai/gpt-oss-20b\"\n",
    "llama3_3_model = OpenAIChatCompletionsModel(model=open_ai_model, openai_client=groq_client)\n",
    "\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model = llama3_3_model\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\", \n",
    "    model = llama3_3_model\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent], \n",
    "    model = llama3_3_model)\n",
    "\n",
    "\n",
    "\n",
    "query = input(\"Ask your query .................    \")\n",
    "print(\"question : \",query)\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(triage_agent, query)\n",
    "    print(\"answer : \",result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950c200",
   "metadata": {},
   "source": [
    "#### OpenAi Agent SDK Development Add a guardrail with handoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e12fafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Response: The first president of the United States was **George Washington**. He was inaugurated on April 30, 1789, and served two terms until 1797.\n",
      "\n",
      "### Why he was chosen\n",
      "- **Reputation and leadership:** Washington had led the Continental Army to victory in the Revolutionary War and was widely respected for his integrity and military skill.\n",
      "- **Neutrality:** As a relatively young nation, the United States needed a president who could appeal to a broad range of factions and maintain political stability.\n",
      "- **Constitutional guidance:** The newly drafted Constitution set up a system of checks and balances; Washington’s leadership helped define the executive’s role within that framework.\n",
      "\n",
      "### Key actions during his presidency\n",
      "- **Establishing precedents:** Washington set many of the protocols still used today—such as appointing a cabinet, issuing the first executive orders, and laying out a neutral foreign policy stance.\n",
      "- **Neutrality Proclamation (1793):** Declared the U.S. would not take sides in the conflict between Britain and France, helping keep the young nation out of European wars.\n",
      "- **Farewell Address (1796):** Warned against political parties and entangling alliances, influencing American political thought for decades.\n",
      "\n",
      "Washington’s presidency laid the foundations for the U.S. government and helped shape the nation’s early direction.\n",
      "🤖 Response: To find the square of a number, you multiply the number by itself.\n",
      "\n",
      "**Step‑by‑step for 2:**\n",
      "\n",
      "1. Take the number 2.  \n",
      "2. Multiply it by itself:  \n",
      "   \\[\n",
      "   2 \\times 2 = 4\n",
      "   \\]\n",
      "3. The result is 4.\n",
      "\n",
      "So, **the square of 2 is 4**.\n",
      "\n",
      "---\n",
      "\n",
      "### Quick examples to reinforce the idea\n",
      "\n",
      "| Number | Square (Number × Number) | Result |\n",
      "|--------|--------------------------|--------|\n",
      "| 1      | \\(1 \\times 1\\)           | 1      |\n",
      "| 3      | \\(3 \\times 3\\)           | 9      |\n",
      "| 5      | \\(5 \\times 5\\)           | 25     |\n",
      "\n",
      "In each case, you just multiply the number by itself to get the square.\n",
      "🤖 Deny Response  : I’m sorry, but I can’t help with that.  \n",
      "I’m only able to provide assistance on math and history topics.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, InputGuardrail, GuardrailFunctionOutput, Runner, InputGuardrailTripwireTriggered\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"You are asked to check if this is query is related to (Math,History) query. Answer clearly in text: 'Yes – reason...' or 'No – reason...'\",\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events and context clearly.\",\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "\n",
    "send_deny_response_agent = Agent(\n",
    "    name=\"Deny replier\",\n",
    "    instructions=\"You are responsible to the write deny response message of query if not related to 'Math and History', reply polietly, and also write reason why you can not help with that query. \",\n",
    "    model=llama3_3_model)\n",
    "\n",
    "\n",
    "\n",
    "async def guardrail_function(ctx, agent, user_input):\n",
    "    result = await Runner.run(guardrail_agent, user_input, context=ctx.context)\n",
    "    raw = result.final_output.strip().lower()\n",
    "    # print(raw)\n",
    "    is_valid_query = raw.startswith(\"yes\")\n",
    "    reasoning = result.final_output.strip()\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info={\"is_homework\": is_valid_query, \"reasoning\": reasoning},\n",
    "        tripwire_triggered=not is_valid_query\n",
    "    )\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"Route homework questions to the right tutor, get response frome the expert tutor.\",\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    input_guardrails=[InputGuardrail(guardrail_function=guardrail_function)],\n",
    "    model=llama3_3_model\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    for q in [\"who was the first president of the united states?\", \"what is the square of 2?\",\"Name of bollywood 3 khan superstar?\"]:\n",
    "        try:\n",
    "            res = await Runner.run(triage_agent, q)\n",
    "            print(f\"🤖 Response: {res.final_output}\")\n",
    "        except InputGuardrailTripwireTriggered:\n",
    "            # Use the guardrail result's reasoning to craft a reply\n",
    "            res = await Runner.run(send_deny_response_agent,f\"User is asked this question : {q}\") #  <---- \n",
    "            print(f\"🤖 Deny Response  : {res.final_output}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f8100",
   "metadata": {},
   "source": [
    "#### openAi Agent with Tool configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e710277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s the current weather in **Mumbai**:\n",
      "\n",
      "- **Temperature:** 27.5 °C (≈81 °F)  \n",
      "- **Humidity:** 80 %  \n",
      "- **Conditions:** Overcast clouds  \n",
      "\n",
      "Feel free to let me know if you’d like a forecast for the next few days or weather for another city!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import requests\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from agents import Agent, OpenAIChatCompletionsModel, Runner, function_tool, set_tracing_disabled\n",
    "\n",
    "set_tracing_disabled(disabled=True)\n",
    "\n",
    "API_KEY = \"25f0559351f3c75faa45800ee42df5b1\"  # 🔑 Replace with your key\n",
    "BASE_URL = \"https://api.openweathermap.org/data/2.5/weather\"  # login signup and create api\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": API_KEY,\n",
    "        \"units\": \"metric\"  # Use \"imperial\" for Fahrenheit\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        # print(data)\n",
    "        if response.status_code == 200:\n",
    "            # print({\n",
    "            #     \"city\": data[\"name\"],\n",
    "            #     \"temperature\": data[\"main\"][\"temp\"],\n",
    "            #     \"humidity\": data[\"main\"][\"humidity\"],\n",
    "            #     \"description\": data[\"weather\"][0][\"description\"].capitalize()\n",
    "            # })\n",
    "            return {\n",
    "                \"city\": data[\"name\"],\n",
    "                \"temperature\": data[\"main\"][\"temp\"],\n",
    "                \"humidity\": data[\"main\"][\"humidity\"],\n",
    "                \"description\": data[\"weather\"][0][\"description\"].capitalize()\n",
    "            }\n",
    "        else:\n",
    "            return {\"error\": data.get(\"message\", \"Unable to fetch weather data\")}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # This agent will use the custom LLM provider\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"You are responsive to provide the whether information to a specific, using tool.\",\n",
    "        model=  llama3_3_model,  \n",
    "        tools=[get_weather],\n",
    "    )\n",
    "\n",
    "    result = await Runner.run(agent, \"What's the weather in mummai?\")\n",
    "    print(result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188a647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
